<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Feiran Wang</title>
    <meta name="author" content="Feiran Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Homepage of Feiran Wang.">
    <meta name="keywords" content="Feiran Wang, Ph.D., 3D Vision">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="canonical" href="https://brack-wang.github.io/">
    <link rel="icon" href="image/me/iconf2_small.jpg">
  </head>

  <body>
    <div class="container">

      <!-- Bio Section -->
      <div class="bio-section">
        <div class="bio-text">
          <h1 class="site-name">Feiran Wang</h1>
          <p>
            Hi, I'm Feiran, a third-year PhD student in computer vision at <a href="https://www.iit.edu/">Illinois Tech</a>, advised by Professor <a href="https://tomyan555.github.io/">Yan Yan</a>. I'm currently visiting <a href="https://umich.edu/">University of Michigan, Ann Arbor</a>.
          </p>
          <p>
            My research focuses on integrating the physical world into the digital domain and creating real-world impact, with particular emphasis on 3D reconstruction, vision foundation models, medical imaging, and generative AI.
          </p>
          <p>
            I hold a M.S. from <a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a>, advised by Professor <a href="http://luthuli.cs.uiuc.edu/~daf/">David Forsyth</a>, and a B.S. from <a href="https://en.shu.edu.cn/">Shanghai University</a>, advised by Professor <a href="https://scholar.google.com/citations?user=JGm4z4YAAAAJ&hl=zh-CN">Xiaoqiang Li</a>. Previously, I had academic visits at the <a href="https://www.utoronto.ca/">University of Toronto</a> and <a href="https://www.uic.edu/">University of Illinois Chicago</a>.
          </p>
          <div class="social-links">
            <a href="mailto:wfr2099@gmail.com">Email</a>
            <a href="data/Feiran_Wang_CV.pdf">CV</a>
            <a href="https://github.com/Brack-Wang">Github</a>
            <a href="https://www.linkedin.com/in/feiran-wang-3d/">Linkedin</a>
            <a href="https://scholar.google.com/citations?user=jnNYwF0AAAAJ&hl=en&oi=sra">Scholar</a>
          </div>
        </div>
        <div class="bio-photo">
          <a href="image/me/me_small.jpg">
            <img alt="profile photo" src="image/me/self_small.jpg">
          </a>
        </div>
      </div>

      <!-- News Section -->
      <h2 class="section-heading">News</h2>
      <ul class="news-list">
        <li class="news-highlight">
          Actively looking for summer internship 2026! View my <a href="data/Feiran_Wang_CV.pdf" target="_blank" style="font-weight:bold;">Resume</a>
        </li>
        <li><em>2026.02</em> One paper, CIF, is accepted to CVPR 2026 ðŸŽ‰</li>
        <li><em>2026.01</em> One first-author paper, CogniMap3D, is accepted to ICLR 2026 ðŸŽ‰</li>
        <li><em>2026.01</em> Started my Visiting Scholar position at the University of Michigan, Ann Arbor.</li>
        <li><em>2026.01</em> Our research on long-term memory reconstruction, CogniMap3D, is available on arXiv.</li>
        <li><em>2025.12</em> Code for X-Field is now publicly available.</li>
        <li><em>2025.09</em> One first-author paper, X-Field, is accepted to NeurIPS 2025 as Spotlight (top 3%) ðŸŽ‰</li>
        <li><em>2025.03</em> Our work on 3D medical image generation, ZECO, is accepted to MVA 2025 (Oral). ðŸŽ‰</li>
      </ul>

      <!-- Publication Section -->
      <h2 class="section-heading">Publication</h2>
      <div class="pub-list">

        <!-- CIF -->
        <div class="pub-item">
          <div class="pub-thumb">
            <div class="video-overlay" id="cif_video">
              <video width="100%" height="100%" muted autoplay loop playsinline webkit-playsinline>
                <source src="image/cif/CIF_small_compat.mp4" type="video/mp4">
              </video>
            </div>
            <img src="image/cif/CIF_small_poster.jpg" alt="CIF">
          </div>
          <div class="pub-info">
            <a href="https://arxiv.org/pdf/2512.14126" class="pub-title">Consistent Instance Field for Dynamic Scene Understanding</a>
            <div class="pub-authors">
              <a href="https://adreamwu.github.io/">Junyi Wu</a>,
              <a href="https://nv-nguyen.github.io/">Van Nguyen Nguyen</a>,
              ...,
              <strong>Feiran Wang</strong>,
              <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>,
              <a href="https://tomyan555.github.io/">Yan Yan</a>,
              <a href="https://wuziyan.com/">Ziyan Wu</a>
            </div>
            <div class="pub-venue"><em>CVPR</em>, 2026</div>
            <div class="pub-links">
              <a href="https://adreamwu.github.io/Consistent-Instance-Field/">project page</a>
              <span class="sep">/</span>
              <a href="https://arxiv.org/abs/2512.14126">paper</a>
            </div>
            <p class="pub-desc">
              CIF formulates a continuous probabilistic field over object existence and identity in space-time, enabling consistent instance representations across views for dynamic scene understanding.
            </p>
          </div>
        </div>

        <!-- RayMap3R -->
        <div class="pub-item">
          <div class="pub-thumb">
            <div class="video-overlay" id="raymap_video">
              <video width="100%" height="100%" muted autoplay loop playsinline webkit-playsinline>
                <source src="image/raymap3r/raymapteaser_small_compat.mp4" type="video/mp4">
              </video>
            </div>
            <img src="image/raymap3r/raymapteaser_small_poster.jpg" alt="RayMap3R">
          </div>
          <div class="pub-info">
            <a href="" class="pub-title">RayMap3R: Inference-Time RayMap for Dynamic 3D Reconstruction</a>
            <div class="pub-authors">
              <strong>Feiran Wang</strong>,
              <a href="https://tomyan555.github.io/">Yan Yan</a>
            </div>
            <div class="pub-venue"><em>Under Review</em></div>
            <p class="pub-desc">
              We revisit and observe that RayMap-based predictions exhibit inherent static scene bias and propose RayMap3R, a training free streaming framework for dynamic scene reconstruction.
            </p>
          </div>
        </div>

        <!-- CogniMap3D -->
        <div class="pub-item">
          <div class="pub-thumb">
            <div class="video-overlay" id="cognimap_video">
              <video width="100%" height="100%" muted autoplay loop playsinline webkit-playsinline>
                <source src="image/cognimap/cognimap_cropped_compat.mp4" type="video/mp4">
              </video>
            </div>
            <img src="image/cognimap/cognimap_cropped_poster.jpg" alt="CogniMap3D">
          </div>
          <div class="pub-info">
            <a href="https://arxiv.org/pdf/2601.08175" class="pub-title">CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval</a>
            <div class="pub-authors">
              <strong>Feiran Wang</strong>,
              <a href="https://adreamwu.github.io/">Junyi Wu</a>,
              <a href="https://www.cai-lab.org/home">Dawen Cai</a>,
              <a href="https://yhongcs.github.io/">Yuan Hong</a>,
              <a href="https://tomyan555.github.io/">Yan Yan</a>
            </div>
            <div class="pub-venue"><em>ICLR</em>, 2026</div>
            <div class="pub-links">
              <a href="https://arxiv.org/pdf/2601.08175">paper</a>
              <span class="sep">/</span>
              <a href="https://github.com/Brack-Wang/cognimap3D">code</a>
            </div>
            <p class="pub-desc">
              We present CogniMap3D, a bioinspired framework that maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval.
            </p>
          </div>
        </div>

        <!-- X-Field -->
        <div class="pub-item">
          <div class="pub-thumb">
            <div class="video-overlay" id="xfield_video">
              <video width="100%" height="100%" muted autoplay loop playsinline webkit-playsinline>
                <source src="image/xfield/xfvideo_small_compat.mp4" type="video/mp4">
              </video>
            </div>
            <img src="image/xfield/xfvideo_small_poster.jpg" alt="X-Field">
          </div>
          <div class="pub-info">
            <a href="https://brack-wang.github.io/XField/" class="pub-title">X-Field: A Physically Informed Representation for 3D X-ray Reconstruction</a>
            <div class="pub-authors">
              <strong>Feiran Wang</strong>,
              <a href="https://scholar.google.com/citations?user=NsYqGP8AAAAJ&hl=en">Jiachen Tao</a><sup>*</sup>,
              <a href="https://adreamwu.github.io/">Junyi Wu</a><sup>*</sup>,
              <a href="https://scholar.google.com/citations?user=04SJDVoAAAAJ&hl=en">Bin Duan</a>,
              <a href="https://kaiwang960112.github.io/">Kai Wang</a>,
              <a href="https://z-x-yang.github.io/">Zongxin Yang</a>,
              <a href="https://tomyan555.github.io/">Yan Yan</a>
            </div>
            <div class="pub-venue"><em>NeurIPS</em>, 2025 <strong>(Spotlight)</strong></div>
            <div class="pub-links">
              <a href="https://brack-wang.github.io/XField/">project page</a>
              <span class="sep">/</span>
              <a href="https://arxiv.org/abs/2503.08596">paper</a>
              <span class="sep">/</span>
              <a href="https://github.com/Brack-Wang/X-Field">code</a>
            </div>
            <p class="pub-desc">
              Rooted in the X-ray imaging process, X-Field presents a representation specifically for high-quality X-ray Novel View Synthesis and CT Reconstruction.
            </p>
          </div>
        </div>

        <!-- ZECO -->
        <div class="pub-item">
          <div class="pub-thumb">
            <div class="video-overlay" id="zeco_video">
              <video width="100%" height="100%" muted autoplay loop playsinline webkit-playsinline>
                <source src="image/zeco/zeco4_small_compat.mp4" type="video/mp4">
              </video>
            </div>
            <img src="image/zeco/zeco4_small_poster.jpg" alt="ZECO">
          </div>
          <div class="pub-info">
            <a href="https://brack-wang.github.io/ZECO_web/" class="pub-title">ZECO: ZeroFusion Guided 3D MRI Conditional Generation</a>
            <div class="pub-authors">
              <strong>Feiran Wang</strong>,
              <a href="https://scholar.google.com/citations?user=04SJDVoAAAAJ&hl=en">Bin Duan</a>,
              <a href="https://scholar.google.com/citations?user=NsYqGP8AAAAJ&hl=en">Jiachen Tao</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=Von4NncAAAAJ&view_op=list_works&sortby=pubdate">Nikhil Sharma</a>,
              <a href="https://www.cai-lab.org/home">Dawen Cai</a>,
              <a href="https://tomyan555.github.io/">Yan Yan</a>
            </div>
            <div class="pub-venue"><em>MVA</em>, 2025 <strong>(Oral)</strong></div>
            <div class="pub-links">
              <a href="https://brack-wang.github.io/ZECO_web/">project page</a>
              <span class="sep">/</span>
              <a href="https://arxiv.org/abs/2503.18246">paper</a>
              <span class="sep">/</span>
              <a href="https://github.com/Brack-Wang/ZECO">code</a>
            </div>
            <p class="pub-desc">
              To mitigate of medical data scarcity, ZECO synthesizes high-quality 3D MRI images across various modalities, conditioned on segmentation masks.
            </p>
          </div>
        </div>

        <!-- PCCN-RE -->
        <div class="pub-item">
          <div class="pub-thumb">
            <div class="video-overlay" id="pccn_video">
              <video width="100%" height="100%" muted autoplay loop playsinline webkit-playsinline>
                <source src="image/pccn/pccn_small_compat.mp4" type="video/mp4">
              </video>
            </div>
            <img src="image/pccn/pccn_small_poster.jpg" alt="PCCN-RE">
          </div>
          <div class="pub-info">
            <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cvi2.12112" class="pub-title">PCCN-RE: Point Cloud Colourisation Network Based on Relevance Embedding</a>
            <div class="pub-authors">
              <strong>Feiran Wang</strong>,
              Jitao Liu,
              <a href="https://scholar.google.com/citations?user=JGm4z4YAAAAJ&hl=en">Xiaoqiang Li</a>
            </div>
            <div class="pub-venue"><em>IET Computer Vision</em>, 2022</div>
            <p class="pub-desc">
              Point Clouds captured by Lidar are often colorless, PCCN-RE enables high-quality colorization with a relevance embedding module on Conditional GAN.
            </p>
          </div>
        </div>

      </div>

      <!-- Scientific Project Section -->
      <h2 class="section-heading">Scientific Project</h2>
      <div class="pub-list">

        <!-- Neuron -->
        <div class="pub-item">
          <div class="pub-thumb">
            <div class="video-overlay" id="neuron_video">
              <video width="100%" height="100%" muted autoplay loop playsinline webkit-playsinline>
                <source src="image/neuron/neuron_small_compat.mp4" type="video/mp4">
              </video>
            </div>
            <img src="image/neuron/neuron_small_poster.jpg" alt="Neuron">
          </div>
          <div class="pub-info">
            <a href="https://brack-wang.github.io/brainbow_webpage/" class="pub-title">A Unified Framework for Unsupervised Sparse-to-dense Brain Image Generation and Neural Circuit Reconstruction</a>
            <p class="pub-desc">
              Understanding morphology and distribution of neurons remains a significant challenge in modern neuroscience. We aim to develop a unified framework for sparse-to-dense neural generation and unsupervised segmentation, providing deeper insights into neural activity and connectivity.
            </p>
          </div>
        </div>

      </div>

      <!-- Article Section -->
      <h2 class="section-heading">Article</h2>
      <div class="article-card">
        <a href="blog/3D_scene_11_22_25.html" class="pub-title">
          Why 3D Scenes May Emerge as a Transformative Modality in Human Communication
        </a>
        <div class="article-meta">November 2025 &middot; 5 min read</div>
        <p class="article-desc">
          An analysis of why 3D scenes may become the next major communication modality, examining the technological convergence and infrastructure developments that suggest we're approaching a transformative inflection point.
        </p>
      </div>

      <!-- Award Section -->
      <h2 class="section-heading">Award</h2>
      <div class="award-item">
        <div class="award-icon">
          <img src="image/award/tang.png" alt="Cyrus Tang Foundation">
        </div>
        <div class="award-info">
          <a href="https://www.cyrustangfoundation.org/" target="_blank">Cyrus Tang Scholarship</a>
          <span class="award-date">(Jan 2024 - Dec 2025)</span>
          <br>
          Recognized for advancing medical imaging research and contributions to the healthcare community.
        </div>
      </div>

      <!-- Footer -->
      <div class="site-footer">
        Design from <a href="https://github.com/jonbarron/website">Jon Barron's web</a>
      </div>

    </div>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
      document.querySelectorAll('video').forEach(function(v) {
        v.muted = true;
        v.play();
      });
    });
    </script>
  </body>
</html>
